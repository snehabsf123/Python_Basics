{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "\n",
        "Ensemble Learning in machine learning is a technique where multiple models (often called “weak learners”) are combined to solve a problem and improve overall performance.\n",
        "\n",
        "The key idea behind ensemble learning is that a group of models working together performs better than a single model alone. By combining predictions from several models, the ensemble can reduce errors due to bias, variance, or noise in the data.\n",
        "\n",
        "Ensemble methods can be broadly categorized into:\n",
        "\n",
        "Bagging (Bootstrap Aggregating) – builds multiple independent models on random subsets of data and averages their predictions (e.g., Random Forest).\n",
        "\n",
        "Boosting – builds models sequentially, where each new model tries to correct the errors of the previous ones (e.g., AdaBoost, XGBoost).\n",
        "\n",
        "Stacking – combines different types of models and uses another model to learn how to best combine their predictions.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-b4fobnA3mxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bagging (Bootstrap Aggregating):\n",
        "\n",
        "Bagging builds multiple models independently using random subsets of the training data.\n",
        "\n",
        "Each model gets a bootstrap sample (random sampling with replacement) of the original dataset.\n",
        "\n",
        "The final prediction is made by averaging (for regression) or majority voting (for classification).\n",
        "\n",
        "Bagging reduces variance and prevents overfitting.\n",
        "\n",
        "Example: Random Forest builds multiple decision trees independently and combines their outputs.\n",
        "\n",
        "Boosting:\n",
        "\n",
        "Boosting builds models sequentially, where each new model focuses on the errors made by previous models.\n",
        "\n",
        "It assigns higher weights to misclassified samples so that subsequent models correct them.\n",
        "\n",
        "The final prediction is a weighted combination of all models.\n",
        "\n",
        "Boosting reduces bias and can improve accuracy, but may overfit if too many models are used.\n",
        "\n",
        "Example: AdaBoost or XGBoost, where each weak learner improves on the mistakes of the previous ones.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eqsIrWNW36bC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "\n",
        "Bootstrap sampling is a statistical technique where we create multiple new datasets from the original dataset by randomly sampling with replacement. This means that some samples may appear multiple times in a bootstrap sample, while others may be left out.\n",
        "\n",
        "In Bagging methods like Random Forest, bootstrap sampling plays a key role:\n",
        "\n",
        "It allows each decision tree in the ensemble to be trained on a different subset of the data, increasing diversity among trees.\n",
        "\n",
        "By training on different samples, trees become less correlated, which improves the ensemble’s overall performance.\n",
        "\n",
        "Combining predictions from multiple trees (through majority voting for classification or averaging for regression) reduces variance and overfitting, making the model more robust.\n",
        "\n",
        "Example:\n",
        "If we have a dataset with 100 samples, a bootstrap sample might randomly select 100 samples with replacement. Some original samples may appear twice, while others may not appear at all. Each tree in a Random Forest uses a different bootstrap sample, and their predictions are aggregated to give the final output.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "IvXwAhkZ4KIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "\n",
        "Out-of-Bag (OOB) samples are the data points not included in a bootstrap sample when creating a tree in Bagging methods like Random Forest.\n",
        "\n",
        "Since each tree is trained on a bootstrap sample, roughly one-third of the original dataset is left out of that sample. These left-out samples are called OOB samples.\n",
        "\n",
        "OOB score is an internal evaluation metric that uses these OOB samples to estimate the model’s performance without needing a separate validation set:\n",
        "\n",
        "For each tree, predictions are made on its OOB samples.\n",
        "\n",
        "Each data point may be an OOB sample for several trees, so the majority vote (classification) or average prediction (regression) across those trees is taken.\n",
        "\n",
        "Comparing these predictions with the true labels gives the OOB accuracy or error.\n",
        "\n",
        "Advantages of OOB score:\n",
        "\n",
        "Provides an unbiased estimate of model performance.\n",
        "\n",
        "Saves the need for a separate validation set, which is useful when the dataset is small.\n",
        "\n",
        "Example:\n",
        "In a Random Forest with 100 trees and 1,000 data points:\n",
        "\n",
        "Each tree is trained on ~1,000 bootstrap samples.\n",
        "\n",
        "About 333 points are left out for each tree as OOB samples.\n",
        "\n",
        "Aggregating predictions for all OOB samples gives the OOB score, which approximates the test accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "goikdzZC4WCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "\n",
        "Single Decision Tree:\n",
        "\n",
        "Feature importance is calculated based on how much each feature reduces impurity (e.g., Gini or Entropy) in that tree.\n",
        "\n",
        "Importance depends entirely on the structure of that single tree.\n",
        "\n",
        "Sensitive to data variations; small changes in data can lead to very different feature rankings.\n",
        "\n",
        "Can overemphasize features that happen to appear near the top of the tree.\n",
        "\n",
        "Provides easy-to-interpret importance but may be unstable and biased.\n",
        "\n",
        "Random Forest:\n",
        "\n",
        "Feature importance is averaged over all the trees in the forest, making it more robust and reliable.\n",
        "\n",
        "Reduces variance compared to a single tree because multiple trees are used.\n",
        "\n",
        "Can handle high-dimensional data better and reduces the impact of outliers.\n",
        "\n",
        "Less biased towards features with many categories or extreme values.\n",
        "\n",
        "Provides a more stable and generalizable measure of feature importance.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ktYpKsJg4kYE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-KEPdnQO408p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame to display features and their importance\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort features by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5cEjbdc4Cjs",
        "outputId": "f1eb4f1c-d9eb-4deb-f2a4-96d8d4a67844"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vRcOzZGR5MCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees (updated parameter name)\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # use 'estimator' instead of 'base_estimator'\n",
        "    n_estimators=50,                     # Number of trees\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy of Single Decision Tree: {accuracy_dt:.2f}\")\n",
        "print(f\"Accuracy of Bagging Classifier: {accuracy_bag:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrtWYDBn5OEC",
        "outputId": "05b273c5-ab3a-4f0d-d0c4-0ec59b4de8b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.00\n",
            "Accuracy of Bagging Classifier: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5Tbjc5oI5jnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 2, 4, 6]\n",
        "}\n",
        "\n",
        "# Initialize Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform GridSearchCV to tune hyperparameters\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Train the final model with best parameters\n",
        "final_rf = RandomForestClassifier(\n",
        "    n_estimators=best_params['n_estimators'],\n",
        "    max_depth=best_params['max_depth'],\n",
        "    random_state=42\n",
        ")\n",
        "final_rf.fit(X_train, y_train)\n",
        "y_pred = final_rf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Final Accuracy on Test Set: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdiiPCay5pFF",
        "outputId": "0a8f7b4a-cefc-4df1-c2c5-617d5030a036"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final Accuracy on Test Set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kidzopjx5yXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Bagging Regressor with Decision Trees (updated parameter name)\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),  # use 'estimator' instead of 'base_estimator'\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print Mean Squared Errors\n",
        "print(f\"Mean Squared Error of Bagging Regressor: {mse_bag:.4f}\")\n",
        "print(f\"Mean Squared Error of Random Forest Regressor: {mse_rf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqECE7Le540S",
        "outputId": "dc8a9ae9-f847-4ce0-fa9e-72b5de3f9218"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor: 0.2579\n",
            "Mean Squared Error of Random Forest Regressor: 0.2565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "1. Choose between Bagging or Boosting\n",
        "\n",
        "Bagging (e.g., Random Forest) is preferred when the base model has high variance, such as decision trees, and you want to reduce overfitting.\n",
        "\n",
        "Boosting (e.g., AdaBoost, XGBoost, LightGBM) is preferred when the base model has high bias or the dataset is complex, and you want to improve predictive accuracy by sequentially correcting errors.\n",
        "\n",
        "Step: Start by analyzing the data and training a few base models. If individual models overfit, use Bagging. If they underfit, use Boosting.\n",
        "\n",
        "2. Handle Overfitting\n",
        "\n",
        "Use techniques such as:\n",
        "\n",
        "Limiting tree depth (max_depth) to prevent trees from memorizing training data.\n",
        "\n",
        "Setting a minimum number of samples per leaf (min_samples_leaf) to avoid tiny splits.\n",
        "\n",
        "Using regularization in boosting (e.g., learning rate in XGBoost).\n",
        "\n",
        "Feature selection to remove irrelevant or noisy predictors.\n",
        "\n",
        "Cross-validation to monitor model performance on unseen data.\n",
        "\n",
        "3. Select Base Models\n",
        "\n",
        "Decision Trees are commonly used as base models for both Bagging and Boosting because they capture non-linear relationships.\n",
        "\n",
        "For tabular financial data, tree-based models like Random Forest, XGBoost, or LightGBM are highly effective.\n",
        "\n",
        "Optionally, stacking can combine different base models (e.g., logistic regression, trees, SVM) to leverage complementary strengths.\n",
        "\n",
        "4. Evaluate Performance using Cross-Validation\n",
        "\n",
        "Use k-fold cross-validation (e.g., k=5 or 10) to ensure that model performance is consistent across multiple subsets of the data.\n",
        "\n",
        "Evaluate using metrics suitable for imbalanced datasets like ROC-AUC, precision, recall, and F1-score, rather than just accuracy, since defaults are usually rare.\n",
        "\n",
        "Step: For each fold, train the ensemble on the training subset and test on the validation subset, then average metrics across folds.\n",
        "\n",
        "5. Justify How Ensemble Learning Improves Decision-Making\n",
        "\n",
        "Reduces errors: Combining multiple models reduces both variance and bias, producing more reliable predictions.\n",
        "\n",
        "Robust predictions: Ensembles handle noisy, incomplete, or complex financial data better than single models.\n",
        "\n",
        "Risk assessment: More accurate predictions of loan default allow the institution to make better lending decisions, reduce non-performing loans, and optimize interest rates.\n",
        "\n",
        "Regulatory compliance: Transparent ensemble models like Random Forest allow feature importance analysis, which helps in explaining decisions for audits.\n",
        "\n"
      ],
      "metadata": {
        "id": "NdHJPRqf6X_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Load dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# XGBoost Regressor\n",
        "xgb = XGBRegressor(random_state=42)\n",
        "scores = cross_val_score(xgb, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "print(\"Average MSE:\", -scores.mean())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZCbDWcx6kp4",
        "outputId": "b82caf1d-8c62-43ff-bdc2-232608fdb2bb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE: 0.23472186477097723\n"
          ]
        }
      ]
    }
  ]
}