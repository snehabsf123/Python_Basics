{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "\n",
        "Information Gain measures how much uncertainty (entropy) is reduced after splitting a dataset based on a particular feature.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "It tells us which feature gives the most “information” about the target variable.\n",
        "\n",
        "Entropy (Measure of Impurity)\n",
        "\n",
        "Before understanding Information Gain, we need Entropy, which measures how mixed the data is.\n",
        "\n",
        "Entropy(S) = - Summation (p(i) log2 p(i))\n",
        "\n",
        "Entropy = 0 → perfectly pure\n",
        "\n",
        "Entropy = 1 → highly impure (for binary classification)\n",
        "\n",
        "Information Gain Formula\n",
        "\n",
        "IG(S,A)= Entropy(S) - Summation (Sv)/(S) x Entropy(S)\n",
        "\n",
        "Where:\n",
        "\n",
        "S = original dataset\n",
        "A = feature used for splitting\n",
        "Sv= subset of data where feature A has value v\n",
        "\n",
        "How Information Gain is Used in Decision Trees\n",
        "\n",
        "Calculate entropy of the entire dataset\n",
        "\n",
        "For each feature:\n",
        "\n",
        "Split the dataset based on feature values\n",
        "\n",
        "Calculate entropy for each split\n",
        "\n",
        "Compute Information Gain\n",
        "\n",
        "Select the feature with the highest Information Gain\n",
        "\n",
        "Repeat the process recursively for child nodes\n",
        "\n",
        "This approach is commonly used in ID3 and C4.5 decision tree algorithms.\n",
        "\n",
        "Example (Conceptual)\n",
        "\n",
        "If splitting on “Weather” reduces uncertainty more than splitting on “Temperature”, then “Weather” will be chosen as the root node.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UNhYdIgFuMaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "Hint: Directly compares the two main impurity measures, highlighting strengths,\n",
        "weaknesses, and appropriate use cases.\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "Gini Impurity measures the probability that a randomly selected data point would be misclassified based on the node’s class distribution.\n",
        "\n",
        "Entropy measures the amount of uncertainty or randomness in the data using concepts from information theory.\n",
        "\n",
        "Gini Impurity is computationally faster because it does not involve logarithmic calculations.\n",
        "\n",
        "Entropy is computationally more expensive due to the use of logarithms.\n",
        "\n",
        "Gini Impurity is less sensitive to small changes in class probabilities.\n",
        "\n",
        "Entropy is more sensitive to changes, especially when nodes are near pure.\n",
        "\n",
        "Gini Impurity is commonly used in the CART decision tree algorithm (e.g., in scikit-learn).\n",
        "\n",
        "Entropy is commonly used in ID3 and C4.5 decision tree algorithms.\n",
        "\n",
        "Gini Impurity is preferred for large datasets where speed is important.\n",
        "\n",
        "Entropy is preferred when theoretical interpretability and balanced splits are more important.\n",
        "\n",
        "Practical Insight\n",
        "\n",
        "In practice, both often produce very similar trees, and performance differences are usually negligible. Choice often depends on:\n",
        "\n",
        "Algorithm used\n",
        "\n",
        "Dataset size\n",
        "\n",
        "Computational constraints\n",
        "\n",
        "\n",
        "Appropriate Use Cases\n",
        "\n",
        "Scenario\t------------------------------------------------- Preferred Measure\n",
        "\n",
        "Large datasets / speed critical\t------------------------------Gini Impurity\n",
        "\n",
        "Need for interpretability / theory-------------------------------Entropy\n",
        "\n",
        "Standard ML libraries----------------------------------------Gini Impurity\n",
        "\n",
        "Academic or conceptual models\t-------------------------------- Entropy\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gpAf61kYvsQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "Pre-pruning in Decision Trees is a technique used to stop the tree from growing too deep during training in order to prevent overfitting.\n",
        "\n",
        "It works by setting stopping conditions in advance, so the tree does not split a node if certain criteria are not met. Instead of growing the full tree and cutting it back later, pre-pruning controls complexity while the tree is being built.\n",
        "\n",
        "Common pre-pruning criteria include:\n",
        "\n",
        "Limiting the maximum depth of the tree\n",
        "\n",
        "Setting a minimum number of samples required to split a node\n",
        "\n",
        "Setting a minimum number of samples required in a leaf node\n",
        "\n",
        "Requiring a minimum information gain or impurity decrease for a split\n",
        "\n",
        "The main advantage of pre-pruning is that it reduces overfitting, training time, and model complexity. However, if the stopping rules are too strict, it can lead to underfitting, where the model fails to capture important patterns in the data.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2_vsFo0Ww2EA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n",
        "Hint: Use criterion='gini' in DecisionTreeClassifier and access .feature_importances_.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "H2TbB_B5velG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create and train the Decision Tree model using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Display feature importances with feature names\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': iris.feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "print(feature_importance_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VAqRmoLx0KE",
        "outputId": "fe9dcc92-6314-451d-aa05-db7ee4920027"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Feature  Importance\n",
            "0  sepal length (cm)    0.013333\n",
            "1   sepal width (cm)    0.000000\n",
            "2  petal length (cm)    0.564056\n",
            "3   petal width (cm)    0.422611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression.\n",
        "\n",
        "It works by finding an optimal decision boundary called a hyperplane.\n",
        "\n",
        "The goal of SVM is to maximize the margin between different classes.\n",
        "\n",
        "The margin is the distance between the hyperplane and the closest data points.\n",
        "\n",
        "These closest data points are known as support vectors.\n",
        "\n",
        "Maximizing the margin helps improve model generalization.\n",
        "\n",
        "SVM can handle linearly separable data.\n",
        "\n",
        "It can also handle non-linearly separable data using kernel functions.\n",
        "\n",
        "Common kernels include linear, polynomial, and radial basis function (RBF).\n",
        "\n",
        "SVM performs well in high-dimensional feature spaces and is resistant to overfitting.\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose we want to classify emails as Spam or Not Spam.\n",
        "\n",
        "Each email is represented using features such as word frequency and email length.\n",
        "\n",
        "SVM finds a line (in 2D) or a plane (in higher dimensions) that best separates spam and non-spam emails.\n",
        "\n",
        "The emails closest to the separating line become the support vectors.\n",
        "\n",
        "By maximizing the distance from these emails, SVM creates a robust classifier.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lT6_3f1myC2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "---\n",
        "Answer:\n",
        "The Kernel Trick is a technique used in Support Vector Machines to handle non-linearly separable data.\n",
        "\n",
        "It allows SVM to transform data into a higher-dimensional space where separation is possible.\n",
        "\n",
        "Instead of explicitly computing this transformation, the kernel trick computes inner products directly.\n",
        "\n",
        "This makes computation efficient even in very high or infinite dimensions.\n",
        "A kernel function measures similarity between pairs of data points.\n",
        "\n",
        "Using kernels, SVM can create non-linear decision boundaries in the original space.\n",
        "\n",
        "Common kernel functions include Linear, Polynomial, and Radial Basis Function (RBF).\n",
        "\n",
        "The kernel trick enables SVM to solve complex classification problems efficiently.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wAPu3v9XyfMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "31M07W48zFKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train SVM with Linear Kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Train SVM with RBF Kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print accuracies\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwN4CLeEx9fJ",
        "outputId": "31ae07cb-4590-42e2-ec4b-a15e16ed3f82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.98\n",
            "Accuracy of SVM with RBF Kernel: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Answer:\n",
        "The Naïve Bayes classifier is a probabilistic machine learning algorithm used for classification tasks.\n",
        "\n",
        "It is based on Bayes’ Theorem, which calculates the probability of a class given the features.\n",
        "\n",
        "The formula is:\n",
        "\n",
        "P(class|Features) = P(Features|class) x P(class) / P(features)\n",
        "\n",
        "It is called “Naïve” because it assumes that all features are independent of each other, even though in real-world data this is rarely true.\n",
        "\n",
        "This “naïve” assumption simplifies computation and allows the model to work efficiently on high-dimensional data.\n",
        "\n",
        "Example:\n",
        "Suppose we want to classify an email as Spam or Not Spam.\n",
        "\n",
        "Features could include words like “offer”, “win”, or “free”.\n",
        "\n",
        "Naïve Bayes calculates the probability of an email being spam based on the presence of these words, assuming each word contributes independently to the final decision.\n",
        "\n",
        "Despite the independence assumption, Naïve Bayes often performs surprisingly well in text classification, email filtering, and sentiment analysis.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ktDAs17dzbdd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve\n",
        "Bayes, and Bernoulli Naïve Bayes\n",
        "\n",
        "\n",
        "---\n",
        "Answer:\n",
        "***Gaussian Naïve Bayes:***\n",
        "\n",
        "Assumes that the features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Used for continuous numerical data.\n",
        "\n",
        "Calculates probabilities using the mean and variance of each feature.\n",
        "\n",
        "Example:\n",
        "\n",
        "Predicting whether a person has diabetes based on continuous features like blood sugar or BMI.\n",
        "\n",
        "***Multinomial Naïve Bayes:***\n",
        "\n",
        "Designed for discrete count data, such as word counts in text classification.\n",
        "\n",
        "Assumes features represent the frequency or occurrence of events.\n",
        "\n",
        "Commonly used in document classification or spam detection.\n",
        "\n",
        "Example:\n",
        "\n",
        "Classifying emails as spam based on the number of times certain words appear.\n",
        "\n",
        "***Bernoulli Naïve Bayes:***\n",
        "\n",
        "Works with binary/boolean features, representing presence or absence.\n",
        "\n",
        "Each feature is either 0 (absent) or 1 (present).\n",
        "\n",
        "Often used in text classification with binary word occurrence.\n",
        "\n",
        "Example:\n",
        "\n",
        " Determining if an email is spam based on whether certain keywords are present or not, ignoring frequency.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9qvGqU-80G2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nO4P35qn0rHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train the Gaussian Naïve Bayes model\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Gaussian Naïve Bayes classifier: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "-iDpJxfr0ykR",
        "outputId": "9d61d8b7-38e6-47d0-e619-ddd3b84a7e3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Gaussian Naïve Bayes classifier: 0.94\n"
          ]
        }
      ]
    }
  ]
}